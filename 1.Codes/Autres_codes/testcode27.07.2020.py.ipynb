{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.0.0-alpha0\n",
      "The tensorboard module is not an IPython extension.\n"
     ]
    }
   ],
   "source": [
    "#INSTALATION DES DEPENDANCES\n",
    "\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\"\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DEFINIR LE MODELE\n",
    "\n",
    "# le reseau de neurones va tenté de predire les X et les Y en sortie\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, x, y):\n",
    "      super(Model, self).__init__()\n",
    "\n",
    "# CREATION DES INPUTS\n",
    "\n",
    "      # création de X, c'est une constante de type \"float\"\n",
    "      self.x_input = tf.constant(x, dtype=tf.float64)\n",
    "      # création de Y, c'est une constante de type \"float64\"\n",
    "      self.y_input = tf.constant(y, dtype=tf.float64)\n",
    "\n",
    "#CREATION DE LA 1ere COUCHE 2x2 (layer1)\n",
    "\n",
    "      # Création du poids d'entrée W1, c est une variable, on prends 2 neuronnes en entrée et créer 2 neurones, la valeur minimale et 0 et la valeur max                est 1. type float. le trainable permet d'ajouter de la descente de gradient et son nom est W1\n",
    "      self.W1 = tf.Variable(tf.random.uniform([2,2], minval=0., maxval=1., dtype=tf.float64), trainable = True, name='W1')\n",
    "\n",
    "      # Création du poids d'entrée b1, c est une variable, on prends 2 neuronnes en entrée\n",
    "      self.b1 = tf.Variable(tf.random.uniform([2], minval=0., maxval=1., dtype=tf.float64), trainable = True, name='b1')\n",
    "\n",
    "\n",
    "#CREATION DE LA 2eme COUCHE 2x1 (layer2)\n",
    "\n",
    "    # Création du poids d'entrée W2, c est une variable, on prends 2 neuronnes en entrée et créer 1 neurones,\n",
    "      self.W2 = tf.Variable(tf.random.uniform([2,1], minval=0., maxval=1., dtype=tf.float64), trainable = True, name='W2')\n",
    "      \n",
    "    # Création du poids d'entrée B2, c est une variable, on prends 1neuronnes en entrée\n",
    "      self.b2 = tf.Variable(tf.random.uniform([1], minval=0., maxval=1., dtype=tf.float64), trainable = True, name='b2')\n",
    "\n",
    "\n",
    "# CREATION DE FONCTIONS DE CALCUL\n",
    "# appel du reseau de neuronnes\n",
    "\n",
    "\n",
    "    # definition d'un input constant de type float \n",
    "    def call(self, inputs):\n",
    "      inputs = tf.constant(inputs, dtype=tf.float64)\n",
    "   # tf.linalg.matmul est une multiplication \n",
    "   # tf.add est une adition \n",
    "      in_neurons_hidden_layer = tf.add(tf.linalg.matmul(inputs, self.W1),self.b1)  # J1= x*W1+b1\n",
    "    # calcul de l output \n",
    "    # tf.sigmoid est le calcul de la sigmoid \n",
    "      out_neurons_hidden_layer = tf.sigmoid(in_neurons_hidden_layer)\n",
    "\n",
    "    # calcul de l output layer\n",
    "\n",
    "      in_neurons_output_layer = tf.add(tf.linalg.matmul(out_neurons_hidden_layer, self.W2),self.b2) # J2= x*W2+b2\n",
    "\n",
    "      out_neurons_output_layer = tf.sigmoid(in_neurons_output_layer)\n",
    "\n",
    "      return out_neurons_output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input1</th>\n",
       "      <th>input2</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   input1  input2  output\n",
       "0     0.0     0.0     0.0\n",
       "1     1.0     1.0     0.0\n",
       "2     0.0     1.0     1.0\n",
       "3     1.0     0.0     1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apprentissage du modele\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#XOR pour expliquer un reseau de neuronnes \n",
    "x = np.array([[0.,0.],[1.,1.],[0.,1.],[1.,0.]])\n",
    "y = np.array([0.,0.,1.,1.])\n",
    "\n",
    "pd.DataFrame({\n",
    "    'input1':np.vstack(x).T[0],\n",
    "    'input2': np.vstack(x).T[1],\n",
    "    'output':y\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at the epoch  0 tf.Tensor(5.5435032760416645, shape=(), dtype=float64)\n",
      "loss at the epoch  100 tf.Tensor(4.0022872912374545, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "#!rm -rf ./logs/ \n",
    "#Create logs\n",
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "\n",
    "# creer un fichier pour Tensorboard pour monitorer les neuronnes \n",
    "summary_writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "model = Model(x, y)\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.9)\n",
    "\n",
    "\n",
    "# DEFINITION DE LA DESCENTE DE GRADIENT\n",
    "\n",
    "\n",
    "# cette fonction est la difference entre le resultat et ce que lon attends \n",
    "def loss(outputs_model, targets):\n",
    "  # soustraction de la valeure de l'on attends par rapport au modele\n",
    "  error = tf.math.subtract(targets,outputs_model)\n",
    "  return tf.reduce_sum(tf.square(error))\n",
    "\n",
    "\n",
    "# cette fonction est la descente de gradiant \n",
    "def get_gradient(model, inputs, targets):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss_value = loss(model(inputs), targets)\n",
    "  return tape.gradient(loss_value, [model.W1, model.b1, model.W2, model.b2])\n",
    "\n",
    "\n",
    "\n",
    "# cette fonction est la boucle du modele\n",
    "def run_network(inputs, targets, epochs):\n",
    "  for i in range(epochs):\n",
    "    grads=get_gradient(model, inputs, targets)\n",
    "\n",
    "# nous allons envoyé une liste ZIP de nos gradiants\n",
    "    optimizer.apply_gradients(zip(grads, [model.W1, model.b1, model.W2, model.b2]))\n",
    "    # calcul des erreurs pour les afficher \n",
    "    loss_epoch = loss(model(inputs), model.y_input)\n",
    "    with summary_writer.as_default():\n",
    "      # afficher l'erreur\n",
    "      tf.summary.scalar('loss', loss_epoch, step=i)\n",
    "\n",
    "    if i % 100 == 0 :\n",
    "      print(\"loss at the epoch \", i , loss_epoch) \n",
    "       \n",
    "      \n",
    "      \n",
    "\n",
    "# le run_network compile tout \n",
    "# epochs est le nombre de fois que l on fait la boucle du modele\n",
    "run_network(x,y,epochs=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
